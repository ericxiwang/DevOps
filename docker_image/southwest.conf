dataflow {
  delete = true
}
device {
  checkInterval = 3000
  timeout = 10
}
sdk {
  timeout = 10
  deployTimeout = 10
  scriptPath = /usr/bin/lua
  validate = "/usr/share/server/bin/validate.lua"
  suffix = ".lua"
  folder = "/data/sdk"
}
kafka {
  brokerList = "127.0.0.1:9092"
}
zookeeper {
  zookeeperList = "127.0.0.1:2181"
  connectionTimeout = 30
  sessionTimeout = 30
  node_path = /cornerstone1/nodes
  policy_path = /cornerstone1/policies
  signature_path = /cornerstone1/signatures
}
rules {
  responseTimeoutInSeconds = 30
}
env {
  python = "/usr/bin/python"
}
port {
  path = "/sys/class/net/eth0/carrier"
  op_path = "/sys/class/net/eth0/operstate"
}

network {
  ftp_interface="eth0"
}

incident {
    mergeIntervalInSeconds = 30
    concurrentThreads = 10
    maximumQueueSize = 10
}

spark {
  executor_uri = "/usr/local/spark-1.6.1-bin-hadoop2.6.tgz"
  mesos_executor_home = "/usr/local/spark-1.6.1-bin-hadoop2.6"
  scheduler_mode = "FAIR"
  mesos_coarse = "false"
  driver_allowMultipleContexts = "true"
  executor_extraClassPath = "/usr/local/spark-cassandra-connector-java-assembly-1.5.0.jar"
  cassandra_connection_host = "127.0.0.1"
  master = "127.0.0.1:2181"
  app_lib_path = "/usr/local/spark-1.6.1-bin-hadoop2.6/bin"
}

cassandra {
    clusterName = ""
    contactPoints = ["localhost"]
}

db {
  initPath = "com/terminal/dataservice/"
}

service_config {
    enabled = false
    package_disabled = ["terminal"]
    service_disabled = ["com.northstar.actor.IEventBus"]
}